{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cc99ce-ebc5-4afd-833b-3f6c89f818f3",
   "metadata": {},
   "source": [
    "## HW2: Running the crawler and saving results to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc933a-a8a3-4514-aa5a-f26dd4fbc051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "keywords = [\n",
    "    'Python', 'JavaScript', 'Java', 'C#', 'C++', 'Ruby', 'Go', 'Swift', 'TypeScript', \n",
    "    'PHP', 'Kotlin', 'Rust', 'R', 'Scala', 'HTML5', 'CSS3', 'React', 'Angular', 'Vue.js', \n",
    "    'Node.js', 'Django', 'Flask', 'Ruby on Rails', 'ASP.NET', 'Spring Boot', 'Next.js', \n",
    "    'TensorFlow', 'PyTorch', 'Scikit-learn', 'Keras', 'Pandas', 'NumPy', 'Matplotlib', \n",
    "    'Seaborn', 'Jupyter Notebook', 'Apache Spark', 'Hadoop', 'Apache Kafka', 'SQL', \n",
    "    'NoSQL', 'MongoDB', 'Cassandra', 'Elasticsearch', 'Tableau', 'Power BI', 'Docker', \n",
    "    'Kubernetes', 'Jenkins'\n",
    "]\n",
    "\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n",
    "]\n",
    "\n",
    "# Define a list of other header components to simulate different users\n",
    "accept_headers = [\n",
    "    'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8'\n",
    "]\n",
    "\n",
    "languages = [\n",
    "    'en-US,en;q=0.5',\n",
    "    'en-GB,en;q=0.5',\n",
    "    'en-CA,en;q=0.5'\n",
    "]\n",
    "\n",
    "referers = [\n",
    "    'https://www.google.com/',\n",
    "    'https://www.bing.com/',\n",
    "    'https://www.yahoo.com/'\n",
    "]\n",
    "\n",
    "def extract_requirements(link, count):\n",
    "    if count >= 10:\n",
    "        return \"none found\"\n",
    "    try:\n",
    "        headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}\n",
    "        data = requests.get(link,headers = headers)\n",
    "        soup_job_details = BeautifulSoup(data.text, 'html.parser')\n",
    "        description = soup_job_details.find('div',class_='description__text').find('section').find('div').find_all('ul')\n",
    "        # Look for all ul's\n",
    "        requirements = [] \n",
    "        for ul in description:\n",
    "            requirements.extend(ul.find_all('li'))\n",
    "        requirements = [i.text for i in requirements]\n",
    "\n",
    "        values = []\n",
    "        for li in requirements:\n",
    "            if li.find(\"years\") != -1 or li.find(\"experience\") != -1 or li.find(\"Experience\") != -1:\n",
    "                values.append(li)\n",
    "        return values\n",
    "    except Exception as e:\n",
    "        return extract_requirements(link, count+1)\n",
    "\n",
    "links = []\n",
    "experience = []\n",
    "i = 0\n",
    "num_results = 60\n",
    "\n",
    "word = \"tensorflow\"\n",
    "\n",
    "example_search_link = f'https://www.linkedin.com/jobs/search/?currentJobId=3957224827&distance=25&geoId=101620260&keywords={word}&origin=JOBS_HOME_KEYWORD_HISTORY&refresh=true&position=3&pageNum=0'\n",
    "headers = {\n",
    "    'User-Agent': random.choice(user_agents),\n",
    "    'Accept': random.choice(accept_headers),\n",
    "    'Accept-Language': random.choice(languages),\n",
    "    'Referer': random.choice(referers),\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Accept-Encoding': 'gzip, deflate, br'\n",
    "}\n",
    "data = requests.get(example_search_link,headers = headers)\n",
    "soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "columns = ['Job Link'] + [f'Job Requirement {i}' for i in range(1, 11)]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "link = soup.find('ul',class_='jobs-search__results-list').find_all('li')\n",
    "for li in link:\n",
    "    li = li.find('a')['href']\n",
    "    exp_i = extract_requirements(li, 0)\n",
    "    if len(exp_i) > 0:\n",
    "        links.append(li)\n",
    "        experience.append(exp_i)\n",
    "        # Create a single row with the job link and requirements\n",
    "        data = [li] + exp_i + [None] * (10 - len(exp_i))\n",
    "    \n",
    "        # Append the data to the DataFrame\n",
    "        df.loc[i] = data\n",
    "        if i >= num_results - 1:\n",
    "            break\n",
    "        i += 1\n",
    "\n",
    "df.to_csv('job_requirements.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f24ed4-9d72-4dab-b869-a59409db9625",
   "metadata": {},
   "source": [
    "## Finding most common words and creating inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd7884-1791-4580-b252-e92176ba5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for requirements in experience:\n",
    "    for req in requirements:\n",
    "        words = re.findall(r'\\b\\w+\\b', req.lower())\n",
    "        words = [word for word in words if word not in stop]\n",
    "        all_words.extend(words)\n",
    "\n",
    "word_counter = Counter(all_words)\n",
    "\n",
    "most_common_words = word_counter.most_common(15)\n",
    "print(\"15 most common words:\", most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36c0bf-2e59-4427-afdc-8d921139d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the inverted index\n",
    "inverted_index = {word: [] for word, count in most_common_words}\n",
    "\n",
    "# Populate the inverted index\n",
    "for i, requirements in enumerate(experience):\n",
    "    for req in requirements:\n",
    "        words = re.findall(r'\\b\\w+\\b', req.lower())\n",
    "        for word in words:\n",
    "            if word in inverted_index:\n",
    "                if i not in inverted_index[word]:\n",
    "                    inverted_index[word].append(i)\n",
    "\n",
    "# Print the inverted index\n",
    "for word in inverted_index:\n",
    "    print(f\"{word}: {inverted_index[word]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66b16e8-5a0a-43e0-9590-c1d05567e9ff",
   "metadata": {},
   "source": [
    "## Creating TF-IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e19a5a-a32d-4be2-9d21-dde658128b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Assuming 'experience' is a list of lists of job requirements and 'links' is a list of job links\n",
    "\n",
    "# Flatten the list of job requirements into a list of strings\n",
    "documents = [' '.join(requirements) for requirements in experience]\n",
    "\n",
    "# Step 1: Create a list of all unique words\n",
    "all_words = set()\n",
    "for document in documents:\n",
    "    words = re.findall(r'\\b\\w+\\b', document.lower())\n",
    "    all_words.update(words)\n",
    "\n",
    "all_words = sorted(all_words)  # Sort the list of all unique words\n",
    "\n",
    "# Step 2: Calculate Term Frequency (TF)\n",
    "def compute_tf(document, all_words):\n",
    "    tf_dict = {word: 0 for word in all_words}\n",
    "    words = re.findall(r'\\b\\w+\\b', document.lower())\n",
    "    total_words = len(words)\n",
    "    word_counts = Counter(words)\n",
    "    for word, count in word_counts.items():\n",
    "        tf_dict[word] = count / total_words\n",
    "    return tf_dict\n",
    "\n",
    "tf_list = [compute_tf(doc, all_words) for doc in documents]\n",
    "\n",
    "# Step 3: Calculate Inverse Document Frequency (IDF)\n",
    "def compute_idf(documents, all_words):\n",
    "    N = len(documents)\n",
    "    idf_dict = {word: 0 for word in all_words}\n",
    "    for document in documents:\n",
    "        words = set(re.findall(r'\\b\\w+\\b', document.lower()))\n",
    "        for word in words:\n",
    "            if word in idf_dict:\n",
    "                idf_dict[word] += 1\n",
    "    for word, count in idf_dict.items():\n",
    "        idf_dict[word] = math.log(N / (count)) if count != 0 else 0\n",
    "    return idf_dict\n",
    "\n",
    "idf_dict = compute_idf(documents, all_words)\n",
    "\n",
    "# Display TF and IDF\n",
    "tf_df = pd.DataFrame(tf_list, index=links).fillna(0)\n",
    "idf_df = pd.DataFrame(list(idf_dict.items()), columns=['Term', 'IDF']).set_index('Term')\n",
    "\n",
    "print(\"Term Frequency (TF):\")\n",
    "print(tf_df)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "print(idf_df)\n",
    "\n",
    "# Step 4: Calculate TF-IDF\n",
    "def compute_tfidf(tf, idf):\n",
    "    tfidf = {word: tf_val * idf[word] for word, tf_val in tf.items()}\n",
    "    return tfidf\n",
    "\n",
    "tfidf_list = [compute_tfidf(tf, idf_dict) for tf in tf_list]\n",
    "\n",
    "# Display TF-IDF\n",
    "tfidf_df = pd.DataFrame(tfidf_list, index=links).fillna(0)\n",
    "print(\"\\nTF-IDF:\")\n",
    "print(tfidf_df)\n",
    "\n",
    "\n",
    "tf_df.to_csv('TermFrequency.csv')\n",
    "idf_df.to_csv('InverseDocumentFrequency.csv')\n",
    "tfidf_df.to_csv('TF-IDF.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
